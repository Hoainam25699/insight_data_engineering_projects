{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"plops_streaming\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'occupancy'\n",
    "hostname = 'ec2-52-39-242-144.us-west-2.compute.amazonaws.com'\n",
    "url_connect = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)\n",
    "properties = {\"user\":\"spark_user\", \n",
    "              \"password\":os.environ['POSTGRES_PASS'],\n",
    "              \"driver\": \"org.postgresql.Driver\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_id INT, meter_id INT, transaction_id INT, transaction_timestamp STRING, amount_usd INT, usernumber STRING, payment_mean STRING, paid_duration INT, station_id INT, year INT, month INT, vendor STRING'"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "config_schema = OrderedDict()\n",
    "config_schema = [\n",
    "    ('data_id', 'INT'),\n",
    "    ('meter_id', 'INT'),\n",
    "    ('transaction_id', 'INT'),\n",
    "    ('transaction_timestamp', 'STRING'),\n",
    "    ('amount_usd', 'INT'),\n",
    "    ('usernumber', 'STRING'),\n",
    "    ('payment_mean', 'STRING'),\n",
    "    ('paid_duration', 'INT'),\n",
    "    ('station_id', 'INT'),\n",
    "    ('year', 'INT'),\n",
    "    ('month', 'INT'),\n",
    "    ('vendor', 'STRING'),\n",
    "]\n",
    "schema = \", \".join([\"{} {}\".format(col, type) for col, type in config_schema])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data_id: integer (nullable = true)\n",
      " |-- meter_id: integer (nullable = true)\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- transaction_timestamp: string (nullable = true)\n",
      " |-- amount_usd: integer (nullable = true)\n",
      " |-- usernumber: string (nullable = true)\n",
      " |-- payment_mean: string (nullable = true)\n",
      " |-- paid_duration: integer (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- vendor: string (nullable = true)\n",
      "\n",
      "+--------+--------+--------------+---------------------+----------+----------+------------+-------------+----------+----+-----+------+\n",
      "| data_id|meter_id|transaction_id|transaction_timestamp|amount_usd|usernumber|payment_mean|paid_duration|station_id|year|month|vendor|\n",
      "+--------+--------+--------------+---------------------+----------+----------+------------+-------------+----------+----+-----+------+\n",
      "|26060471|19337010|     652611123|  01/12/2019 08:35:42|        19|      NULL| CREDIT CARD|        20780|     10873|2019|    1|  null|\n",
      "+--------+--------+--------------+---------------------+----------+----------+------------+-------------+----------+----+-----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df = spark.read.csv(\n",
    "    \"s3a://project.datasets/transactions/01182019.csv.gz\", header=True, mode=\"PERMISSIVE\", schema=schema\n",
    ")\n",
    "tr_df.printSchema()\n",
    "tr_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_timestamp: timestamp (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- paid_duration: integer (nullable = true)\n",
      " |-- amount_usd: integer (nullable = true)\n",
      " |-- transaction_endtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df = tr_df.select('transaction_timestamp','station_id','paid_duration','amount_usd')\n",
    "tr_df = tr_df.withColumn(\"transaction_timestamp\", F.to_timestamp(tr_df.transaction_timestamp, format=\"MM/dd/yyyy HH:mm:ss\"))\n",
    "tr_df = tr_df.withColumn(\"transaction_endtime\", (F.unix_timestamp(\"transaction_timestamp\") + tr_df.paid_duration).cast('timestamp'))\n",
    "tr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- transaction_starttime: timestamp (nullable = true)\n",
      " |-- transaction_endtime: timestamp (nullable = true)\n",
      " |-- paid_duration: integer (nullable = true)\n",
      " |-- amount_usd: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df = tr_df.withColumnRenamed(\"transaction_timestamp\", \"transaction_starttime\")\n",
    "tr_df = tr_df.select('station_id','transaction_starttime', 'transaction_endtime','paid_duration','amount_usd')\n",
    "tr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"active_transactions\"\n",
    "mode = \"overwrite\"\n",
    "my_writer = DataFrameWriter(tr_df)\n",
    "my_writer.jdbc(url_connect, table, mode, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207274"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.createOrReplaceTempView(\"occupancy_streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- transaction_starttime: timestamp (nullable = true)\n",
      " |-- transaction_endtime: timestamp (nullable = true)\n",
      "\n",
      "+----------+---------------------+-------------------+\n",
      "|station_id|transaction_starttime|transaction_endtime|\n",
      "+----------+---------------------+-------------------+\n",
      "|     10873|  2019-01-12 08:35:42|2019-01-12 14:22:02|\n",
      "+----------+---------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_df.printSchema()\n",
    "tr_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ds = spark.sql(\"select max(transaction_starttime) as latest_timestamp from occupancy_streaming\")\n",
    "min_ds = min_ds.withColumn(\"latest_timestamp\", F.date_trunc('minute', min_ds.latest_timestamp)) \n",
    "\n",
    "min_ds = min_ds.withColumn(\"second_timestamp\", (F.unix_timestamp(min_ds.latest_timestamp) - 60).cast(\"timestamp\")) \\\n",
    "                    .withColumn(\"third_timestamp\", (F.unix_timestamp(min_ds.latest_timestamp) - 120).cast(\"timestamp\")) \\\n",
    "                    .collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-18 23:59:00\n",
      "2019-01-18 23:58:00\n",
      "2019-01-18 23:57:00\n"
     ]
    }
   ],
   "source": [
    "print(min_ds[0][0])\n",
    "print(min_ds[0][1])\n",
    "print(min_ds[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "database_name = 'occupancy'\n",
    "hostname = 'ec2-52-39-242-144.us-west-2.compute.amazonaws.com'\n",
    "url_connect = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)\n",
    "\n",
    "properties = {\"user\":\"spark_user\", \n",
    "              \"password\":os.environ['POSTGRES_PASS'],\n",
    "              \"driver\": \"org.postgresql.Driver\"\n",
    "             }\n",
    "table = \"dim_stations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_df = spark.read.jdbc(url=url_connect, table=table, properties=properties)\n",
    "dim_df = dim_df.select('station_id').withColumn(\"timestamp\", F.lit(min_ds[0][0]))\n",
    "dim_df = dim_df.union(dim_df.withColumn(\"timestamp\", F.lit(min_ds[0][1])))\n",
    "dim_df = dim_df.union(dim_df.withColumn(\"timestamp\", F.lit(min_ds[0][2])))\n",
    "\n",
    "dim_df.printSchema()\n",
    "dim_df.createOrReplaceTempView(\"occupancy_perminute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = (\"SELECT p.station_id, p.timestamp, count(*) AS occupied_spots \"\n",
    "       \"FROM occupancy_perminute p left outer join occupancy_streaming s \"\n",
    "       \"ON p.station_id = s.station_id \"\n",
    "       \"AND p.timestamp BETWEEN s.transaction_starttime AND s.transaction_endtime \"\n",
    "       \"GROUP BY p.station_id, p.timestamp\"\n",
    "      )\n",
    "\n",
    "occupancy_per_minute = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1251"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupancy_per_minute.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"live_occupancy\"\n",
    "mode = \"overwrite\"\n",
    "my_writer = DataFrameWriter(occupancy_per_minute)\n",
    "my_writer.jdbc(url_connect, table, mode, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
