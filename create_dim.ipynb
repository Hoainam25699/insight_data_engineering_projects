{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"plops_edit_dim\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'objectid INT, station_id INT, segkey INT, unitid INT, unitid2 INT, station_address STRING, side STRING, block_id STRING, block_nbr INT, csm STRING, parking_category STRING, load INT, zone INT, total_zones INT, wkd_rate1 DOUBLE, wkd_start1 STRING, wkd_end1 STRING, wkd_rate2 DOUBLE, wkd_start2 STRING, wkd_end2 STRING, wkd_rate3 DOUBLE, wkd_start3 STRING, wkd_end3 STRING, sat_rate1 DOUBLE, sat_start1 STRING, sat_end1 STRING, sat_rate2 DOUBLE, sat_start2 STRING, sat_end2 STRING, sat_rate3 DOUBLE, sat_start3 STRING, sat_end3 STRING, rpz_zone STRING, rpz_area DOUBLE, paidarea STRING, parking_time_limit DOUBLE, subarea STRING, start_time_wkd STRING, end_time_wkd STRING, start_time_sat STRING, end_time_sat STRING, primarydistrictcd STRING, secondarydistrictcd STRING, overrideyn STRING, overridecomment INT, shape_length DOUBLE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "config_schema = OrderedDict()\n",
    "config_schema = [\n",
    "    ('objectid', 'INT'), \n",
    "    ('station_id', 'INT'), \n",
    "    ('segkey', 'INT'), \n",
    "    ('unitid', 'INT'), \n",
    "    ('unitid2', 'INT'), \n",
    "    ('station_address', 'STRING'), \n",
    "    ('side', 'STRING'), \n",
    "    ('block_id', 'STRING'), \n",
    "    ('block_nbr', 'INT'), \n",
    "    ('csm', 'STRING'), \n",
    "    ('parking_category', 'STRING'), \n",
    "    ('load', 'INT'), \n",
    "    ('zone', 'INT'), \n",
    "    ('total_zones', 'INT'), \n",
    "    ('wkd_rate1', 'DOUBLE'), \n",
    "    ('wkd_start1', 'STRING'), \n",
    "    ('wkd_end1', 'STRING'), \n",
    "    ('wkd_rate2', 'DOUBLE'), \n",
    "    ('wkd_start2', 'STRING'), \n",
    "    ('wkd_end2', 'STRING'), \n",
    "    ('wkd_rate3', 'DOUBLE'), \n",
    "    ('wkd_start3', 'STRING'), \n",
    "    ('wkd_end3', 'STRING'), \n",
    "    ('sat_rate1', 'DOUBLE'), \n",
    "    ('sat_start1', 'STRING'), \n",
    "    ('sat_end1', 'STRING'), \n",
    "    ('sat_rate2', 'DOUBLE'), \n",
    "    ('sat_start2', 'STRING'), \n",
    "    ('sat_end2', 'STRING'), \n",
    "    ('sat_rate3', 'DOUBLE'), \n",
    "    ('sat_start3', 'STRING'), \n",
    "    ('sat_end3', 'STRING'), \n",
    "    ('rpz_zone', 'STRING'), \n",
    "    ('rpz_area', 'DOUBLE'), \n",
    "    ('paidarea', 'STRING'), \n",
    "    ('parking_time_limit', 'DOUBLE'), \n",
    "    ('subarea', 'STRING'), \n",
    "    ('start_time_wkd', 'STRING'), \n",
    "    ('end_time_wkd', 'STRING'), \n",
    "    ('start_time_sat', 'STRING'), \n",
    "    ('end_time_sat', 'STRING'), \n",
    "    ('primarydistrictcd', 'STRING'), \n",
    "    ('secondarydistrictcd', 'STRING'), \n",
    "    ('overrideyn', 'STRING'), \n",
    "    ('overridecomment', 'INT'), \n",
    "    ('shape_length', 'DOUBLE')\n",
    "]\n",
    "schema = \", \".join([\"{} {}\".format(col, type) for col, type in config_schema])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df = spark.read.csv(\n",
    "    \"s3a://project.datasets/Blockface.csv.gz\", header=True, mode=\"FAILFAST\", schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df = dim_df.drop('objectid',\n",
    "    'segkey',\n",
    "    'unitid',\n",
    "    'unitid2',\n",
    "    'block_id',\n",
    "    'csm',\n",
    "    'load',\n",
    "    'zone',\n",
    "    'total_zones',\n",
    "    'rpz_zone',\n",
    "    'rpz_area',\n",
    "    'paidarea',\n",
    "    'start_time_wkd',\n",
    "    'end_time_wkd',\n",
    "    'start_time_sat',\n",
    "    'end_time_sat',\n",
    "    'primarydistrictcd',\n",
    "    'secondarydistrictcd',\n",
    "    'overrideyn',\n",
    "    'overridecomment',\n",
    "    'shape_length',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+\n",
      "|station_id|     station_address|side|block_nbr|parking_category|wkd_rate1|wkd_start1|wkd_end1|wkd_rate2|wkd_start2|wkd_end2|wkd_rate3|wkd_start3|wkd_end3|sat_rate1|sat_start1|sat_end1|sat_rate2|sat_start2|sat_end2|sat_rate3|sat_start3|sat_end3|parking_time_limit|subarea|\n",
      "+----------+--------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+\n",
      "|    124699|I5 EXPRESS NE 103...|   N|    10200|            None|      0.0|         0|       0|      0.0|         0|       0|      0.0|         0|       0|      0.0|         0|       0|      0.0|         0|       0|      0.0|         0|       0|              null|   null|\n",
      "+----------+--------------------+----+---------+----------------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+---------+----------+--------+------------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occupancy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timestamp string, occupancy INT, blockface_name STRING, side_of_street STRING, station_id INT, time_limit_category STRING, space_count INT, area STRING, subarea STRING, rate STRING, category STRING, location STRING, emptycol1 STRING, emptycol2 STRING, emptycol3 STRING, emptycol4 STRING, emptycol5 STRING'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "config_schema = OrderedDict()\n",
    "config_schema = [\n",
    "    ('timestamp', 'string'),\n",
    "    ('occupancy', 'INT'),\n",
    "    ('blockface_name', 'STRING'),\n",
    "    ('side_of_street', 'STRING'),\n",
    "    ('station_id', 'INT'),\n",
    "    ('time_limit_category', 'STRING'),\n",
    "    ('space_count', 'INT'),\n",
    "    ('area', 'STRING'),\n",
    "    ('subarea', 'STRING'),\n",
    "    ('rate', 'STRING'),\n",
    "    ('category', 'STRING'),\n",
    "    ('location', 'STRING'),\n",
    "    ('emptycol1','STRING'),\n",
    "    ('emptycol2','STRING'),\n",
    "    ('emptycol3','STRING'),\n",
    "    ('emptycol4','STRING'),\n",
    "    ('emptycol5','STRING')\n",
    "]\n",
    "schema = \", \".join([\"{} {}\".format(col, type) for col, type in config_schema])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_df = spark.read.csv(\n",
    "    #\"s3a://project.datasets/2019-Paid-Parking-Occupancy.csv.gz\", header=True, mode=\"DROPMALFORMED\", schema=schema\n",
    "    \"s3a://project.datasets/Last_48_hours.csv\", header=True, mode=\"DROPMALFORMED\", schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_df = oc_df.drop(\"blockface_name\", \n",
    "             \"side_of_street\", \n",
    "             \"time_limit_category\", \n",
    "             \"area\", \n",
    "             \"subarea\",\n",
    "             \"rate\",\n",
    "             \"category\",\n",
    "             \"emptycol1\",\n",
    "             \"emptycol2\",\n",
    "             \"emptycol3\",\n",
    "             \"emptycol4\",\n",
    "             \"emptycol5\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# oc_df = oc_df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\")) # also it works!!\n",
    "oc_df = oc_df.withColumn(\"timestamp\", F.to_timestamp(oc_df.timestamp, format=\"mm/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- occupancy: integer (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- space_count: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    oc_df.createTempView(\"occupancy\")\n",
    "except:\n",
    "    print('aready occupancy is exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_df = spark.sql(\"select t1.station_id, t1.space_count, t1.location from occupancy t1 where t1.timestamp = (select max(t2.timestamp) from occupancy t2 where t1.station_id = t2.station_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_df = oc_df.join(dim_df, oc_df.station_id == dim_df.station_id, \"inner\").drop(dim_df.station_id)\n",
    "dim_df = oc_df.join(dim_df, [\"station_id\"], \"inner\")#.drop(dim_df.station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- space_count: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- station_address: string (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- block_nbr: integer (nullable = true)\n",
      " |-- parking_category: string (nullable = true)\n",
      " |-- wkd_rate1: double (nullable = true)\n",
      " |-- wkd_start1: string (nullable = true)\n",
      " |-- wkd_end1: string (nullable = true)\n",
      " |-- wkd_rate2: double (nullable = true)\n",
      " |-- wkd_start2: string (nullable = true)\n",
      " |-- wkd_end2: string (nullable = true)\n",
      " |-- wkd_rate3: double (nullable = true)\n",
      " |-- wkd_start3: string (nullable = true)\n",
      " |-- wkd_end3: string (nullable = true)\n",
      " |-- sat_rate1: double (nullable = true)\n",
      " |-- sat_start1: string (nullable = true)\n",
      " |-- sat_end1: string (nullable = true)\n",
      " |-- sat_rate2: double (nullable = true)\n",
      " |-- sat_start2: string (nullable = true)\n",
      " |-- sat_end2: string (nullable = true)\n",
      " |-- sat_rate3: double (nullable = true)\n",
      " |-- sat_start3: string (nullable = true)\n",
      " |-- sat_end3: string (nullable = true)\n",
      " |-- parking_time_limit: double (nullable = true)\n",
      " |-- subarea: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df = dim_df.withColumn('effective_date_start', F.to_date(F.lit(\"01/01/2010\"), format=\"mm/dd/yyyy\")) \\\n",
    "                .withColumn('effective_date_end', F.to_date(F.lit(\"01/01/3000\"), format=\"mm/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- space_count: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- station_address: string (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- block_nbr: integer (nullable = true)\n",
      " |-- parking_category: string (nullable = true)\n",
      " |-- wkd_rate1: double (nullable = true)\n",
      " |-- wkd_start1: string (nullable = true)\n",
      " |-- wkd_end1: string (nullable = true)\n",
      " |-- wkd_rate2: double (nullable = true)\n",
      " |-- wkd_start2: string (nullable = true)\n",
      " |-- wkd_end2: string (nullable = true)\n",
      " |-- wkd_rate3: double (nullable = true)\n",
      " |-- wkd_start3: string (nullable = true)\n",
      " |-- wkd_end3: string (nullable = true)\n",
      " |-- sat_rate1: double (nullable = true)\n",
      " |-- sat_start1: string (nullable = true)\n",
      " |-- sat_end1: string (nullable = true)\n",
      " |-- sat_rate2: double (nullable = true)\n",
      " |-- sat_start2: string (nullable = true)\n",
      " |-- sat_end2: string (nullable = true)\n",
      " |-- sat_rate3: double (nullable = true)\n",
      " |-- sat_start3: string (nullable = true)\n",
      " |-- sat_end3: string (nullable = true)\n",
      " |-- parking_time_limit: double (nullable = true)\n",
      " |-- subarea: string (nullable = true)\n",
      " |-- effective_date_start: date (nullable = true)\n",
      " |-- effective_date_end: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrameWriter\n",
    "my_writer = DataFrameWriter(dim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'occupancy'\n",
    "table = \"dim_stations\"\n",
    "hostname = 'ec2-52-39-242-144.us-west-2.compute.amazonaws.com'\n",
    "mode = \"overwrite\"\n",
    "properties = {\"user\":\"spark_user\", \n",
    "              \"password\":os.environ['POSTGRES_PASS'],\n",
    "              \"driver\": \"org.postgresql.Driver\"\n",
    "             }\n",
    "\n",
    "url_connect = \"jdbc:postgresql://{hostname}:5432/{db}\".format(hostname=hostname, db=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_writer.jdbc(url_connect, table, mode, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
